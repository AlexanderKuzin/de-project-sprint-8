# Реализация сервиса push-уведомлений для агрегатора доставки.

### Описание задачи
Агрегатор для доставки еды набирает популярность и вводит новую опцию — подписку. Она открывает для пользователей ряд возможностей, одна из которых — добавлять рестораны в избранное. Только тогда пользователю будут поступать уведомления о специальных акциях с ограниченным сроком действия. Систему, которая поможет реализовать это обновление, вам и нужно будет создать.
Благодаря обновлению рестораны смогут привлечь новую аудиторию, получить фидбэк на новые блюда и акции, продать профицит товара и увеличить продажи в непиковые часы. Акции длятся недолго, всего несколько часов, и часто бывают ситуативными, а значит, важно быстро доставить их кругу пользователей, у которых ресторан добавлен в избранное.  
Система работает так:
  - Ресторан отправляет через мобильное приложение акцию с ограниченным предложением. Например, такое: «Вот и новое блюдо — его нет в обычном меню. Дарим на него скидку 70% до 14:00! Нам важен каждый комментарий о новинке».
  - Сервис проверяет, у кого из пользователей ресторан находится в избранном списке.
  - Сервис формирует заготовки для push-уведомлений этим пользователям о временных акциях. Уведомления будут отправляться только пока действует акция.

### План проекта
Схема работы проекта:
![image](https://pictures.s3.yandex.net/resources/2._Proekt_1663599076.png)

Сервис будет: 
1. читать данные из Kafka с помощью Spark Structured Streaming и Python в режиме реального времени.
Обратите внимание: `restaurant_id` должен соответствовать входной таблице из Postgres, а поля `adv_campaign_datetime_start` и `adv_campaign_datetime_end` — актуальной информации, то есть текущему времени. 
Образец входного сообщения:

`first_message:{"restaurant_id": "123e4567-e89b-12d3-a456-426614174000","adv_campaign_id": "123e4567-e89b-12d3-a456-426614174003","adv_campaign_content": "first campaign","adv_campaign_owner": "Ivanov Ivan Ivanovich","adv_campaign_owner_contact": "iiivanov@restaurant.ru","adv_campaign_datetime_start": 1659203516,"adv_campaign_datetime_end": 2659207116,"datetime_created": 1659131516}`
В этом сообщении: 
`first_message` — ключ. В рамках задачи он может быть произвольным, но для тестирования кода можно использовать номер сообщения.
: — разделитель ключа и сообщения.
Далее идёт тело самого сообщения. 
- "restaurant_id": "123e4567-e89b-12d3-a456-426614174000", — UUID ресторана;
- "adv_campaign_id": "123e4567-e89b-12d3-a456-426614174003", — UUID рекламной кампании;
- "adv_campaign_content": "first campaign", — текст кампании;
- "adv_campaign_owner": "Ivanov Ivan Ivanovich", — сотрудник ресторана, который является владельцем кампании;
- "adv_campaign_owner_contact": "iiivanov@restaurant.ru", — его контакт;
- "adv_campaign_datetime_start": 1659203516, — время начала рекламной кампании в формате timestamp;
- "adv_campaign_datetime_end": 2659207116, — время её окончания в формате timestamp;
- "datetime_created": 1659131516 — время создания кампании в формате timestamp.

2. получать список подписчиков из базы данных Postgres.

3. джойнить данные из Kafka с данными из БД.
4. сохранять в памяти полученные данные, чтобы не собирать их заново после отправки в Postgres или Kafka.
5. отправлять выходное сообщение в Kafka с информацией об акции, пользователе со списком избранного и ресторане, а ещё вставлять записи в Postgres, чтобы впоследствии получить фидбэк от пользователя. Сервис push-уведомлений будет читать сообщения из Kafka и формировать готовые уведомления. Получать и анализировать фидбэк в вашу задачу не входит — этим займутся аналитики.

### Инструкция по выполнению проекта
### Подготовка
#### Шаг 1. Проверить работу потока.
Убедитесь, что получается отправлять и читать данные из топика Kafka. Это нужно для отладки стримингового приложения. 
Для этого с помощью kcat в одном окне терминала начните читать входной топик. В качестве имени входного топика используйте ваш логин_in. Для выходного топика — ваш логин_out. Если топиков на данный момент нет, они автоматически создадутся при отправке сообщения.
С помощью kcat отправьте сообщение в топик, из которого вы читаете сообщения. Напишите входное сообщение в ваш входной топик:
Проверьте, что в терминале, где был запущен консьюмер, отображается отправленное сообщение. Если это не так, то проверьте предыдущие шаги. 

### Реализация приложения
#### Шаг 2. Прочитать данные об акциях из Kafka.
Напишите с помощью PySpark код стриминга для:
- a. чтения сообщений об акциях из Kafka;
- b. вывода сообщений в консоль.

#### Шаг 3. Прочитать данные о подписчиках из Postgres.
Дополните код чтением статичных данных из таблицы Postgres `subscribers_restaurants` и выводом содержимого на консоль.

#### Шаг 4. Преобразование JSON в датафейм. 
Сообщения из Kafka находятся в формате переменных «ключ-значение» (англ. “key-value”). В переменной value лежит содержимое JSON-файла, то есть текст в формате JSON. Вам же для работы нужен датафрейм, который в качестве колонок использует ключи JSON, поэтому придётся сначала десериализовать value в JSON, а затем преобразовать JSON в датафрейм. 
Дополните код, чтобы он преобразовал JSON из value в датафрейм, где колонками будут ключи в формате JSON.
Отфильтруйте сообщения с рекламными кампаниями, для которых текущее время находится в промежутке между временем старта и временем окончания кампании.

#### Шаг 5. Провести JOIN потоковых и статичных данных.
Дополните код, чтобы сджойнить данные из Kafka с данными из Postgres по полю `restaurant_id`.
Добавьте колонку с датой создания — текущей датой. В датафрейме не должно быть повторяющихся колонок.

#### Шаг 6. Отправить результаты JOIN в Postgres для аналитики фидбэка.
Вам нужно отправлять результаты обработки сразу в два стока — Kafka для push-уведомлений и Postgres для аналитики фидбэка. В обоих случаях вам потребуется метод foreachBatch(…). C его помощью вы можете указать, какие функции нужно применить к каждому микробатчу выходных данных. 
Начнём с Postgres. Дополните код функцией, которая будет отправлять данные в Postgres для получения фидбэка. Для этого:
Измените код в теле функции, чтобы записать df в Postgres с полем `feedback`. Данные нужно записывать в локальную БД, для которой вы создавали таблицу `subscribers_feedback`. 
Доработайте запуск стриминга методом foreachBatch(…), который будет вызывать функцию.

#### Шаг 7. Отправить данные, сериализованные в формат JSON, в Kafka для push-уведомлений.
Дополните функцию из прошлого пункта, чтобы добавить ещё один сток — Kafka. 
Создайте датафрейм для отправки в Kafka.
Сериализуйте данные в сообщение формата «ключ-значение» и заполните только value. Для этого сериализуйте данные из датафрейма в JSON и положите JSON в колонку value.
Отправьте сообщения в результирующий топик Kafka без поля feedback.
В вызове foreachBatch(…) ничего менять не нужно.

#### Шаг 8. Персистентность датафрейма.
Сохраните датафрейм в памяти после объединения данных.
После отправки данных в два стока очистите память от датафрейма.
